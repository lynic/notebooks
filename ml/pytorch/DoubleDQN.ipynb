{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-18 05:52:02,175] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHES = 100\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "MEMORY_CAPACITY = 2000\n",
    "TARGET_REPLACE_ITER = 100\n",
    "\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(N_STATES, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, N_ACTIONS),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net = Net()\n",
    "        self.target_net = Net()\n",
    "        if torch.cuda.is_available():\n",
    "            self.eval_net = self.eval_net.cuda()\n",
    "            self.target_net = self.target_net.cuda()\n",
    "        self.memory = deque(maxlen=MEMORY_CAPACITY)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=0.01)\n",
    "    def choose_action(self, state):\n",
    "        x = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            action_values = self.eval_net(x)\n",
    "            action = torch.max(action_values, -1)[1].data[0, 0]\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "    def refresh_target_net(self):\n",
    "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "    def learn(self):\n",
    "        train_data = random.sample(self.memory, BATCH_SIZE)\n",
    "        states = Variable(torch.FloatTensor(list(map(itemgetter(0), train_data))))\n",
    "        actions = Variable(torch.LongTensor(list(map(itemgetter(1), train_data))))\n",
    "        rewards = Variable(torch.FloatTensor(list(map(itemgetter(2), train_data))))\n",
    "        next_states = Variable(torch.FloatTensor(list(map(itemgetter(3), train_data))))\n",
    "        if torch.cuda.is_available():\n",
    "            states = states.cuda()\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            next_states = next_states.cuda()\n",
    "        q_eval = self.eval_net(states).gather(1, actions.unsqueeze(1))\n",
    "        q_next = self.target_net(next_states).detach()\n",
    "        # here is the change\n",
    "        q_eval4next = self.eval_net(next_states).max(-1)[1]\n",
    "        q_target = rewards + GAMMA * q_next.gather(1, q_eval4next)\n",
    "#         q_target = rewards + GAMMA * q_next.max(-1)[0]\n",
    "        loss = self.criterion(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Steps: 10\n",
      "Scores: 10.0\n",
      "**********\n",
      "Epoch: 1\n",
      "Steps: 8\n",
      "Scores: 8.0\n",
      "**********\n",
      "Epoch: 2\n",
      "Steps: 10\n",
      "Scores: 10.0\n",
      "**********\n",
      "Epoch: 3\n",
      "Steps: 9\n",
      "Scores: 9.0\n",
      "**********\n",
      "Epoch: 4\n",
      "Steps: 9\n",
      "Scores: 9.0\n",
      "**********\n",
      "Epoch: 5\n",
      "Steps: 9\n",
      "Scores: 9.0\n",
      "**********\n",
      "Epoch: 6\n",
      "Steps: 10\n",
      "Scores: 10.0\n",
      "**********\n",
      "Epoch: 7\n",
      "Steps: 21\n",
      "Scores: 21.0\n",
      "**********\n",
      "Epoch: 8\n",
      "Steps: 46\n",
      "Scores: 46.0\n",
      "**********\n",
      "Epoch: 9\n",
      "Steps: 24\n",
      "Scores: 24.0\n",
      "**********\n",
      "Epoch: 10\n",
      "Steps: 106\n",
      "Scores: 106.0\n",
      "**********\n",
      "Epoch: 11\n",
      "Steps: 82\n",
      "Scores: 82.0\n",
      "**********\n",
      "Epoch: 12\n",
      "Steps: 263\n",
      "Scores: 263.0\n",
      "**********\n",
      "Epoch: 13\n",
      "Steps: 279\n",
      "Scores: 279.0\n",
      "**********\n",
      "Epoch: 14\n",
      "Steps: 485\n",
      "Scores: 485.0\n",
      "**********\n",
      "Epoch: 15\n",
      "Steps: 527\n",
      "Scores: 527.0\n",
      "**********\n",
      "Epoch: 16\n",
      "Steps: 884\n",
      "Scores: 884.0\n",
      "**********\n",
      "Epoch: 17\n",
      "Steps: 914\n",
      "Scores: 914.0\n",
      "**********\n",
      "Epoch: 18\n",
      "Steps: 3607\n",
      "Scores: 3607.0\n",
      "**********\n",
      "Epoch: 19\n",
      "Steps: 1653\n",
      "Scores: 1653.0\n",
      "**********\n",
      "Epoch: 20\n",
      "Steps: 484\n",
      "Scores: 484.0\n",
      "**********\n",
      "Epoch: 21\n",
      "Steps: 2435\n",
      "Scores: 2435.0\n",
      "**********\n",
      "Epoch: 22\n",
      "Steps: 348\n",
      "Scores: 348.0\n",
      "**********\n",
      "Epoch: 23\n",
      "Steps: 1785\n",
      "Scores: 1785.0\n",
      "**********\n",
      "Epoch: 24\n",
      "Steps: 915\n",
      "Scores: 915.0\n",
      "**********\n",
      "Epoch: 25\n",
      "Steps: 31\n",
      "Scores: 31.0\n",
      "**********\n",
      "Epoch: 26\n",
      "Steps: 9\n",
      "Scores: 9.0\n",
      "**********\n",
      "Epoch: 27\n",
      "Steps: 16\n",
      "Scores: 16.0\n",
      "**********\n",
      "Epoch: 28\n",
      "Steps: 39\n",
      "Scores: 39.0\n",
      "**********\n",
      "Epoch: 29\n",
      "Steps: 57\n",
      "Scores: 57.0\n",
      "**********\n",
      "Epoch: 30\n",
      "Steps: 13\n",
      "Scores: 13.0\n",
      "**********\n",
      "Epoch: 31\n",
      "Steps: 122\n",
      "Scores: 122.0\n",
      "**********\n",
      "Epoch: 32\n",
      "Steps: 169\n",
      "Scores: 169.0\n",
      "**********\n",
      "Epoch: 33\n",
      "Steps: 1236\n",
      "Scores: 1236.0\n",
      "**********\n",
      "Epoch: 34\n",
      "Steps: 1190\n",
      "Scores: 1190.0\n",
      "**********\n",
      "Epoch: 35\n",
      "Steps: 701\n",
      "Scores: 701.0\n",
      "**********\n",
      "Epoch: 36\n",
      "Steps: 805\n",
      "Scores: 805.0\n",
      "**********\n",
      "Epoch: 37\n",
      "Steps: 412\n",
      "Scores: 412.0\n",
      "**********\n",
      "Epoch: 38\n",
      "Steps: 336\n",
      "Scores: 336.0\n",
      "**********\n",
      "Epoch: 39\n",
      "Steps: 1610\n",
      "Scores: 1610.0\n",
      "**********\n",
      "Epoch: 40\n",
      "Steps: 604\n",
      "Scores: 604.0\n",
      "**********\n",
      "Epoch: 41\n",
      "Steps: 2365\n",
      "Scores: 2365.0\n",
      "**********\n",
      "Epoch: 42\n",
      "Steps: 1557\n",
      "Scores: 1557.0\n",
      "**********\n",
      "Epoch: 43\n",
      "Steps: 1288\n",
      "Scores: 1288.0\n",
      "**********\n",
      "Epoch: 44\n",
      "Steps: 287\n",
      "Scores: 287.0\n",
      "**********\n",
      "Epoch: 45\n",
      "Steps: 760\n",
      "Scores: 760.0\n",
      "**********\n",
      "Epoch: 46\n",
      "Steps: 3200\n",
      "Scores: 3200.0\n",
      "**********\n",
      "Epoch: 47\n",
      "Steps: 449\n",
      "Scores: 449.0\n",
      "**********\n",
      "Epoch: 48\n",
      "Steps: 899\n",
      "Scores: 899.0\n",
      "**********\n",
      "Epoch: 49\n",
      "Steps: 323\n",
      "Scores: 323.0\n",
      "**********\n",
      "Epoch: 50\n",
      "Steps: 1668\n",
      "Scores: 1668.0\n",
      "**********\n",
      "Epoch: 51\n",
      "Steps: 1780\n",
      "Scores: 1780.0\n",
      "**********\n",
      "Epoch: 52\n",
      "Steps: 5089\n",
      "Scores: 5089.0\n",
      "**********\n",
      "Epoch: 53\n",
      "Steps: 1612\n",
      "Scores: 1612.0\n",
      "**********\n",
      "Epoch: 54\n",
      "Steps: 1147\n",
      "Scores: 1147.0\n",
      "**********\n",
      "Epoch: 55\n",
      "Steps: 1625\n",
      "Scores: 1625.0\n",
      "**********\n",
      "Epoch: 56\n",
      "Steps: 1599\n",
      "Scores: 1599.0\n",
      "**********\n",
      "Epoch: 57\n",
      "Steps: 1238\n",
      "Scores: 1238.0\n",
      "**********\n",
      "Epoch: 58\n",
      "Steps: 1837\n",
      "Scores: 1837.0\n",
      "**********\n",
      "Epoch: 59\n",
      "Steps: 975\n",
      "Scores: 975.0\n",
      "**********\n",
      "Epoch: 60\n",
      "Steps: 1324\n",
      "Scores: 1324.0\n",
      "**********\n",
      "Epoch: 61\n",
      "Steps: 1437\n",
      "Scores: 1437.0\n",
      "**********\n",
      "Epoch: 62\n",
      "Steps: 1426\n",
      "Scores: 1426.0\n",
      "**********\n",
      "Epoch: 63\n",
      "Steps: 1492\n",
      "Scores: 1492.0\n",
      "**********\n",
      "Epoch: 64\n",
      "Steps: 1351\n",
      "Scores: 1351.0\n",
      "**********\n",
      "Epoch: 65\n",
      "Steps: 705\n",
      "Scores: 705.0\n",
      "**********\n",
      "Epoch: 66\n",
      "Steps: 2451\n",
      "Scores: 2451.0\n",
      "**********\n",
      "Epoch: 67\n",
      "Steps: 921\n",
      "Scores: 921.0\n",
      "**********\n",
      "Epoch: 68\n",
      "Steps: 781\n",
      "Scores: 781.0\n",
      "**********\n",
      "Epoch: 69\n",
      "Steps: 3392\n",
      "Scores: 3392.0\n",
      "**********\n",
      "Epoch: 70\n",
      "Steps: 1198\n",
      "Scores: 1198.0\n",
      "**********\n",
      "Epoch: 71\n",
      "Steps: 2899\n",
      "Scores: 2899.0\n",
      "**********\n",
      "Epoch: 72\n",
      "Steps: 1802\n",
      "Scores: 1802.0\n",
      "**********\n",
      "Epoch: 73\n",
      "Steps: 956\n",
      "Scores: 956.0\n",
      "**********\n",
      "Epoch: 74\n",
      "Steps: 1262\n",
      "Scores: 1262.0\n",
      "**********\n",
      "Epoch: 75\n",
      "Steps: 888\n",
      "Scores: 888.0\n",
      "**********\n",
      "Epoch: 76\n",
      "Steps: 1240\n",
      "Scores: 1240.0\n",
      "**********\n",
      "Epoch: 77\n",
      "Steps: 1310\n",
      "Scores: 1310.0\n",
      "**********\n",
      "Epoch: 78\n",
      "Steps: 1074\n",
      "Scores: 1074.0\n",
      "**********\n",
      "Epoch: 79\n",
      "Steps: 1639\n",
      "Scores: 1639.0\n",
      "**********\n",
      "Epoch: 80\n",
      "Steps: 2490\n",
      "Scores: 2490.0\n",
      "**********\n",
      "Epoch: 81\n",
      "Steps: 1839\n",
      "Scores: 1839.0\n",
      "**********\n",
      "Epoch: 82\n",
      "Steps: 1698\n",
      "Scores: 1698.0\n",
      "**********\n",
      "Epoch: 83\n",
      "Steps: 623\n",
      "Scores: 623.0\n",
      "**********\n",
      "Epoch: 84\n",
      "Steps: 1362\n",
      "Scores: 1362.0\n",
      "**********\n",
      "Epoch: 85\n",
      "Steps: 817\n",
      "Scores: 817.0\n",
      "**********\n",
      "Epoch: 86\n",
      "Steps: 1915\n",
      "Scores: 1915.0\n",
      "**********\n",
      "Epoch: 87\n",
      "Steps: 2078\n",
      "Scores: 2078.0\n",
      "**********\n",
      "Epoch: 88\n",
      "Steps: 741\n",
      "Scores: 741.0\n",
      "**********\n",
      "Epoch: 89\n",
      "Steps: 2026\n",
      "Scores: 2026.0\n",
      "**********\n",
      "Epoch: 90\n",
      "Steps: 1329\n",
      "Scores: 1329.0\n",
      "**********\n",
      "Epoch: 91\n",
      "Steps: 3750\n",
      "Scores: 3750.0\n",
      "**********\n",
      "Epoch: 92\n",
      "Steps: 1302\n",
      "Scores: 1302.0\n",
      "**********\n",
      "Epoch: 93\n",
      "Steps: 1729\n",
      "Scores: 1729.0\n",
      "**********\n",
      "Epoch: 94\n",
      "Steps: 1435\n",
      "Scores: 1435.0\n",
      "**********\n",
      "Epoch: 95\n",
      "Steps: 1361\n",
      "Scores: 1361.0\n",
      "**********\n",
      "Epoch: 96\n",
      "Steps: 1622\n",
      "Scores: 1622.0\n",
      "**********\n",
      "Epoch: 97\n",
      "Steps: 1417\n",
      "Scores: 1417.0\n",
      "**********\n",
      "Epoch: 98\n",
      "Steps: 1732\n",
      "Scores: 1732.0\n",
      "**********\n",
      "Epoch: 99\n",
      "Steps: 1321\n",
      "Scores: 1321.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "env = env.unwrapped\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    print('*'*10)\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    scores = 0.0\n",
    "    while not done:\n",
    "        action = dqn.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        scores += reward\n",
    "\n",
    "        # Revise reward\n",
    "        x, x_dot, theta, theta_dot = next_state\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2\n",
    "\n",
    "        dqn.remember(state, action, reward, next_state)\n",
    "        if len(dqn.memory) > BATCH_SIZE:\n",
    "#             set_trace()\n",
    "            dqn.learn()\n",
    "        if steps%TARGET_REPLACE_ITER == 0:\n",
    "            dqn.refresh_target_net()\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    print('Steps: {}'.format(steps))\n",
    "    print('Scores: {}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
