{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 02:01:47,916] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHES = 100\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "MEMORY_CAPACITY = 2000\n",
    "TARGET_REPLACE_ITER = 100\n",
    "\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(N_STATES, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # here is the change for dueling DQN\n",
    "        self.value_fc = nn.Sequential(\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(256, N_ACTIONS)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        # here is the change for dueling DQN\n",
    "        out_v = self.value_fc(out)\n",
    "        out_a = self.advantage_fc(out)\n",
    "        out = out_v.expand_as(out_a) + (out_a - out_a.mean(1).expand_as(out_a))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net = Net()\n",
    "        self.target_net = Net()\n",
    "        if torch.cuda.is_available():\n",
    "            self.eval_net = self.eval_net.cuda()\n",
    "            self.target_net = self.target_net.cuda()\n",
    "        self.memory = deque(maxlen=MEMORY_CAPACITY)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=0.01)\n",
    "    def choose_action(self, state):\n",
    "        self.eval_net.eval()\n",
    "        x = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            action_values = self.eval_net(x)\n",
    "            action = torch.max(action_values, -1)[1].data[0, 0]\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        self.eval_net.train()\n",
    "        return action\n",
    "    def refresh_target_net(self):\n",
    "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "    def learn(self):\n",
    "        train_data = random.sample(self.memory, BATCH_SIZE)\n",
    "        states = Variable(torch.FloatTensor(list(map(itemgetter(0), train_data))))\n",
    "        actions = Variable(torch.LongTensor(list(map(itemgetter(1), train_data))))\n",
    "        rewards = Variable(torch.FloatTensor(list(map(itemgetter(2), train_data))))\n",
    "        next_states = Variable(torch.FloatTensor(list(map(itemgetter(3), train_data))))\n",
    "        if torch.cuda.is_available():\n",
    "            states = states.cuda()\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            next_states = next_states.cuda()\n",
    "        q_eval = self.eval_net(states).gather(1, actions.unsqueeze(1))\n",
    "        q_next = self.target_net(next_states).detach()\n",
    "        # here is the change for double DQN\n",
    "        q_eval4next = self.eval_net(next_states).max(-1)[1]\n",
    "        q_target = rewards + GAMMA * q_next.gather(1, q_eval4next)\n",
    "#         q_target = rewards + GAMMA * q_next.max(-1)[0]\n",
    "        loss = self.criterion(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Steps: 58\n",
      "Scores: 58.0\n",
      "**********\n",
      "Epoch: 1\n",
      "Steps: 20\n",
      "Scores: 20.0\n",
      "**********\n",
      "Epoch: 2\n",
      "Steps: 102\n",
      "Scores: 102.0\n",
      "**********\n",
      "Epoch: 3\n",
      "Steps: 75\n",
      "Scores: 75.0\n",
      "**********\n",
      "Epoch: 4\n",
      "Steps: 212\n",
      "Scores: 212.0\n",
      "**********\n",
      "Epoch: 5\n",
      "Steps: 170\n",
      "Scores: 170.0\n",
      "**********\n",
      "Epoch: 6\n",
      "Steps: 148\n",
      "Scores: 148.0\n",
      "**********\n",
      "Epoch: 7\n",
      "Steps: 216\n",
      "Scores: 216.0\n",
      "**********\n",
      "Epoch: 8\n",
      "Steps: 246\n",
      "Scores: 246.0\n",
      "**********\n",
      "Epoch: 9\n",
      "Steps: 256\n",
      "Scores: 256.0\n",
      "**********\n",
      "Epoch: 10\n",
      "Steps: 234\n",
      "Scores: 234.0\n",
      "**********\n",
      "Epoch: 11\n",
      "Steps: 562\n",
      "Scores: 562.0\n",
      "**********\n",
      "Epoch: 12\n",
      "Steps: 1859\n",
      "Scores: 1859.0\n",
      "**********\n",
      "Epoch: 13\n",
      "Steps: 385\n",
      "Scores: 385.0\n",
      "**********\n",
      "Epoch: 14\n",
      "Steps: 510\n",
      "Scores: 510.0\n",
      "**********\n",
      "Epoch: 15\n",
      "Steps: 436\n",
      "Scores: 436.0\n",
      "**********\n",
      "Epoch: 16\n",
      "Steps: 1929\n",
      "Scores: 1929.0\n",
      "**********\n",
      "Epoch: 17\n",
      "Steps: 1969\n",
      "Scores: 1969.0\n",
      "**********\n",
      "Epoch: 18\n",
      "Steps: 397\n",
      "Scores: 397.0\n",
      "**********\n",
      "Epoch: 19\n",
      "Steps: 904\n",
      "Scores: 904.0\n",
      "**********\n",
      "Epoch: 20\n",
      "Steps: 774\n",
      "Scores: 774.0\n",
      "**********\n",
      "Epoch: 21\n",
      "Steps: 369\n",
      "Scores: 369.0\n",
      "**********\n",
      "Epoch: 22\n",
      "Steps: 1292\n",
      "Scores: 1292.0\n",
      "**********\n",
      "Epoch: 23\n",
      "Steps: 764\n",
      "Scores: 764.0\n",
      "**********\n",
      "Epoch: 24\n",
      "Steps: 615\n",
      "Scores: 615.0\n",
      "**********\n",
      "Epoch: 25\n",
      "Steps: 1490\n",
      "Scores: 1490.0\n",
      "**********\n",
      "Epoch: 26\n",
      "Steps: 1830\n",
      "Scores: 1830.0\n",
      "**********\n",
      "Epoch: 27\n",
      "Steps: 923\n",
      "Scores: 923.0\n",
      "**********\n",
      "Epoch: 28\n",
      "Steps: 569\n",
      "Scores: 569.0\n",
      "**********\n",
      "Epoch: 29\n",
      "Steps: 943\n",
      "Scores: 943.0\n",
      "**********\n",
      "Epoch: 30\n",
      "Steps: 1161\n",
      "Scores: 1161.0\n",
      "**********\n",
      "Epoch: 31\n",
      "Steps: 258\n",
      "Scores: 258.0\n",
      "**********\n",
      "Epoch: 32\n",
      "Steps: 789\n",
      "Scores: 789.0\n",
      "**********\n",
      "Epoch: 33\n",
      "Steps: 738\n",
      "Scores: 738.0\n",
      "**********\n",
      "Epoch: 34\n",
      "Steps: 2393\n",
      "Scores: 2393.0\n",
      "**********\n",
      "Epoch: 35\n",
      "Steps: 1113\n",
      "Scores: 1113.0\n",
      "**********\n",
      "Epoch: 36\n",
      "Steps: 682\n",
      "Scores: 682.0\n",
      "**********\n",
      "Epoch: 37\n",
      "Steps: 1913\n",
      "Scores: 1913.0\n",
      "**********\n",
      "Epoch: 38\n",
      "Steps: 1105\n",
      "Scores: 1105.0\n",
      "**********\n",
      "Epoch: 39\n",
      "Steps: 1182\n",
      "Scores: 1182.0\n",
      "**********\n",
      "Epoch: 40\n",
      "Steps: 1764\n",
      "Scores: 1764.0\n",
      "**********\n",
      "Epoch: 41\n",
      "Steps: 1469\n",
      "Scores: 1469.0\n",
      "**********\n",
      "Epoch: 42\n",
      "Steps: 1359\n",
      "Scores: 1359.0\n",
      "**********\n",
      "Epoch: 43\n",
      "Steps: 1045\n",
      "Scores: 1045.0\n",
      "**********\n",
      "Epoch: 44\n",
      "Steps: 1561\n",
      "Scores: 1561.0\n",
      "**********\n",
      "Epoch: 45\n",
      "Steps: 652\n",
      "Scores: 652.0\n",
      "**********\n",
      "Epoch: 46\n",
      "Steps: 358\n",
      "Scores: 358.0\n",
      "**********\n",
      "Epoch: 47\n",
      "Steps: 723\n",
      "Scores: 723.0\n",
      "**********\n",
      "Epoch: 48\n",
      "Steps: 832\n",
      "Scores: 832.0\n",
      "**********\n",
      "Epoch: 49\n",
      "Steps: 570\n",
      "Scores: 570.0\n",
      "**********\n",
      "Epoch: 50\n",
      "Steps: 1131\n",
      "Scores: 1131.0\n",
      "**********\n",
      "Epoch: 51\n",
      "Steps: 2819\n",
      "Scores: 2819.0\n",
      "**********\n",
      "Epoch: 52\n",
      "Steps: 1750\n",
      "Scores: 1750.0\n",
      "**********\n",
      "Epoch: 53\n",
      "Steps: 2171\n",
      "Scores: 2171.0\n",
      "**********\n",
      "Epoch: 54\n",
      "Steps: 529\n",
      "Scores: 529.0\n",
      "**********\n",
      "Epoch: 55\n",
      "Steps: 792\n",
      "Scores: 792.0\n",
      "**********\n",
      "Epoch: 56\n",
      "Steps: 710\n",
      "Scores: 710.0\n",
      "**********\n",
      "Epoch: 57\n",
      "Steps: 970\n",
      "Scores: 970.0\n",
      "**********\n",
      "Epoch: 58\n",
      "Steps: 605\n",
      "Scores: 605.0\n",
      "**********\n",
      "Epoch: 59\n",
      "Steps: 869\n",
      "Scores: 869.0\n",
      "**********\n",
      "Epoch: 60\n",
      "Steps: 3382\n",
      "Scores: 3382.0\n",
      "**********\n",
      "Epoch: 61\n",
      "Steps: 1302\n",
      "Scores: 1302.0\n",
      "**********\n",
      "Epoch: 62\n",
      "Steps: 1442\n",
      "Scores: 1442.0\n",
      "**********\n",
      "Epoch: 63\n",
      "Steps: 1456\n",
      "Scores: 1456.0\n",
      "**********\n",
      "Epoch: 64\n",
      "Steps: 1664\n",
      "Scores: 1664.0\n",
      "**********\n",
      "Epoch: 65\n",
      "Steps: 5333\n",
      "Scores: 5333.0\n",
      "**********\n",
      "Epoch: 66\n",
      "Steps: 4831\n",
      "Scores: 4831.0\n",
      "**********\n",
      "Epoch: 67\n",
      "Steps: 996\n",
      "Scores: 996.0\n",
      "**********\n",
      "Epoch: 68\n",
      "Steps: 1019\n",
      "Scores: 1019.0\n",
      "**********\n",
      "Epoch: 69\n",
      "Steps: 1613\n",
      "Scores: 1613.0\n",
      "**********\n",
      "Epoch: 70\n",
      "Steps: 985\n",
      "Scores: 985.0\n",
      "**********\n",
      "Epoch: 71\n",
      "Steps: 760\n",
      "Scores: 760.0\n",
      "**********\n",
      "Epoch: 72\n",
      "Steps: 662\n",
      "Scores: 662.0\n",
      "**********\n",
      "Epoch: 73\n",
      "Steps: 3568\n",
      "Scores: 3568.0\n",
      "**********\n",
      "Epoch: 74\n",
      "Steps: 657\n",
      "Scores: 657.0\n",
      "**********\n",
      "Epoch: 75\n",
      "Steps: 699\n",
      "Scores: 699.0\n",
      "**********\n",
      "Epoch: 76\n",
      "Steps: 1647\n",
      "Scores: 1647.0\n",
      "**********\n",
      "Epoch: 77\n",
      "Steps: 620\n",
      "Scores: 620.0\n",
      "**********\n",
      "Epoch: 78\n",
      "Steps: 1471\n",
      "Scores: 1471.0\n",
      "**********\n",
      "Epoch: 79\n",
      "Steps: 925\n",
      "Scores: 925.0\n",
      "**********\n",
      "Epoch: 80\n",
      "Steps: 1799\n",
      "Scores: 1799.0\n",
      "**********\n",
      "Epoch: 81\n",
      "Steps: 1740\n",
      "Scores: 1740.0\n",
      "**********\n",
      "Epoch: 82\n",
      "Steps: 628\n",
      "Scores: 628.0\n",
      "**********\n",
      "Epoch: 83\n",
      "Steps: 508\n",
      "Scores: 508.0\n",
      "**********\n",
      "Epoch: 84\n",
      "Steps: 908\n",
      "Scores: 908.0\n",
      "**********\n",
      "Epoch: 85\n",
      "Steps: 967\n",
      "Scores: 967.0\n",
      "**********\n",
      "Epoch: 86\n",
      "Steps: 1079\n",
      "Scores: 1079.0\n",
      "**********\n",
      "Epoch: 87\n",
      "Steps: 1091\n",
      "Scores: 1091.0\n",
      "**********\n",
      "Epoch: 88\n",
      "Steps: 1640\n",
      "Scores: 1640.0\n",
      "**********\n",
      "Epoch: 89\n",
      "Steps: 859\n",
      "Scores: 859.0\n",
      "**********\n",
      "Epoch: 90\n",
      "Steps: 1273\n",
      "Scores: 1273.0\n",
      "**********\n",
      "Epoch: 91\n",
      "Steps: 806\n",
      "Scores: 806.0\n",
      "**********\n",
      "Epoch: 92\n",
      "Steps: 1692\n",
      "Scores: 1692.0\n",
      "**********\n",
      "Epoch: 93\n",
      "Steps: 3213\n",
      "Scores: 3213.0\n",
      "**********\n",
      "Epoch: 94\n",
      "Steps: 1276\n",
      "Scores: 1276.0\n",
      "**********\n",
      "Epoch: 95\n",
      "Steps: 1351\n",
      "Scores: 1351.0\n",
      "**********\n",
      "Epoch: 96\n",
      "Steps: 1388\n",
      "Scores: 1388.0\n",
      "**********\n",
      "Epoch: 97\n",
      "Steps: 1109\n",
      "Scores: 1109.0\n",
      "**********\n",
      "Epoch: 98\n",
      "Steps: 1294\n",
      "Scores: 1294.0\n",
      "**********\n",
      "Epoch: 99\n",
      "Steps: 4228\n",
      "Scores: 4228.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "env = env.unwrapped\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    print('*'*10)\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    scores = 0.0\n",
    "    while not done:\n",
    "        action = dqn.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        scores += reward\n",
    "\n",
    "        # Revise reward\n",
    "        x, x_dot, theta, theta_dot = next_state\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2\n",
    "\n",
    "        dqn.remember(state, action, reward, next_state)\n",
    "        if len(dqn.memory) > BATCH_SIZE:\n",
    "#             set_trace()\n",
    "            dqn.learn()\n",
    "        if steps%TARGET_REPLACE_ITER == 0:\n",
    "            dqn.refresh_target_net()\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    print('Steps: {}'.format(steps))\n",
    "    print('Scores: {}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
